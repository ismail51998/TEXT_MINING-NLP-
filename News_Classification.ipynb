{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Atelier 4 : Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tObjectif : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de cet atelier est de découvrir la classification de documents texte à travers le classificateur SVM que nous allons appliquer sur un dataset contenant les news apparus sur le fil de presse Reuters en 1987.\n",
    "\n",
    "Le datset peut être téléchargé à partir de ce lien : https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection. \n",
    "\n",
    "Le dataset est disponible egalement sur nltk :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset est disponible également sur scikitlearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_Annotated_coffee=reuters.categories(reuters.fileids(\"coffee\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tChargement des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous considerons le dataset reuters à partir de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "#recuperation du vocabulaire du corpus\n",
    "vocabulaire=reuters.words()\n",
    "\n",
    "#recuperation de toutes les categories\n",
    "categories=reuters.categories()\n",
    "\n",
    "#recuperation de tous les id des fichiers appartenant à une categorie bien determinée\n",
    "ids_coffe=reuters.fileids(\"coffee\")\n",
    "\n",
    "#recuperation des mots contenus dans les documents d'une categorie bien determinee\n",
    "coffe_words=reuters.words(reuters.fileids(\"coffee\"))\n",
    "\n",
    "#recuperation du texte brut des documents d'une categorie bien determinee\n",
    "cofee_docs=reuters.raw(reuters.fileids(\"coffee\")[0])\n",
    "\n",
    "#recuperation de toutes les autres classe d'un document annoté avec une classe bien determinee\n",
    "classes_Annotated_coffee=reuters.categories(reuters.fileids(\"coffee\"))\n",
    "\n",
    "#recuperer le dataset d'apprentissage\n",
    "train_categories=[ reuters.categories(i) for i in reuters.fileids() if i.startswith('training/')]\n",
    "train_documents = [reuters.raw(i) for i in reuters.fileids() if i.startswith('training/')]\n",
    "\n",
    "#recuperer le dataset de test\n",
    "test_documents=[reuters.raw(i)  for i in reuters.fileids() if i.startswith('test/')]\n",
    "test_categories = [reuters.categories(i) for i in reuters.fileids() if i.startswith('test/')]\n",
    "\n",
    "# recuperer tout le corpus\n",
    "whole_docs=[reuters.raw(i)for i in reuters.fileids()]\n",
    "whole_cats = [ reuters.categories(i) for i in reuters.fileids()]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Récupération des représentations vectorielles en TF-IDF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10788, 90)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "#fit(raw_documents[, y]): Learn vocabulary and idf from training set.\n",
    "#fit_transform(raw_documents[, y]): Learn vocabulary and idf, return document-term matrix.\n",
    "#transform(raw_documents): Transform documents to document-term matrix.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "vect_whole_docs = vectorizer.fit_transform(whole_docs)\n",
    "vect_train_docs = vectorizer.transform(train_documents)\n",
    "vect_test_docs = vectorizer.transform(test_documents)\n",
    "\n",
    "#recuperer des labels uniques pour les categories\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_categories)\n",
    "test_labels = mlb.transform(test_categories)\n",
    "whole_labels = mlb.fit_transform(whole_cats)\n",
    "\n",
    "print(whole_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tClassification avec SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       719\n",
      "           1       1.00      0.43      0.61        23\n",
      "           2       1.00      0.64      0.78        14\n",
      "           3       0.95      0.60      0.73        30\n",
      "           4       0.88      0.39      0.54        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      0.94      0.97        18\n",
      "           7       1.00      0.50      0.67         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.96      0.96      0.96        28\n",
      "          10       1.00      0.78      0.88        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.95      0.71      0.82        56\n",
      "          13       1.00      0.50      0.67        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.92      0.43      0.59        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.91      0.81      0.86       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.85      0.66      0.74        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.99      0.98      0.99      1087\n",
      "          22       1.00      0.20      0.33        10\n",
      "          23       1.00      0.53      0.69        17\n",
      "          24       1.00      0.80      0.89        35\n",
      "          25       0.92      0.73      0.81        30\n",
      "          26       0.98      0.81      0.89       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.33      0.50         6\n",
      "          31       1.00      0.75      0.86         4\n",
      "          32       1.00      0.43      0.60         7\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.88      0.66      0.75       131\n",
      "          35       1.00      0.83      0.91        12\n",
      "          36       0.75      0.64      0.69        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.92      0.57      0.71        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00        14\n",
      "          41       1.00      1.00      1.00         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.69      0.38      0.49        24\n",
      "          44       0.00      0.00      0.00         6\n",
      "          45       1.00      0.16      0.27        19\n",
      "          46       0.81      0.73      0.76       179\n",
      "          47       0.89      0.74      0.81        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.74      0.47      0.57        30\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       1.00      0.17      0.29         6\n",
      "          54       0.79      0.47      0.59        47\n",
      "          55       1.00      0.45      0.62        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.60      0.75        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.00      0.00      0.00        12\n",
      "          60       0.00      0.00      0.00         7\n",
      "          61       1.00      0.33      0.50         3\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.91      0.56      0.69        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       0.88      0.29      0.44        24\n",
      "          69       1.00      0.75      0.86        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.89      0.63      0.74        89\n",
      "          72       0.00      0.00      0.00         8\n",
      "          73       0.75      0.30      0.43        10\n",
      "          74       1.00      0.15      0.27        13\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.83      0.45      0.59        33\n",
      "          77       0.00      0.00      0.00        11\n",
      "          78       0.96      0.72      0.83        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       0.00      0.00      0.00         4\n",
      "          83       1.00      0.58      0.74        12\n",
      "          84       0.87      0.75      0.81       117\n",
      "          85       0.94      0.43      0.59        37\n",
      "          86       0.93      0.75      0.83        71\n",
      "          87       1.00      0.60      0.75        10\n",
      "          88       0.00      0.00      0.00        14\n",
      "          89       1.00      0.46      0.63        13\n",
      "\n",
      "   micro avg       0.95      0.78      0.86      3744\n",
      "   macro avg       0.59      0.36      0.43      3744\n",
      "weighted avg       0.91      0.78      0.83      3744\n",
      " samples avg       0.87      0.85      0.86      3744\n",
      "\n",
      "0.8042398145081153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier_svm = OneVsRestClassifier(LinearSVC())\n",
    "classifier_svm.fit(vect_train_docs,train_labels)\n",
    "test_labels_predict=classifier_svm.predict(vect_test_docs)\n",
    "print(classification_report(test_labels,test_labels_predict))\n",
    "scores=classifier_svm.score(vect_test_docs,test_labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tClassification  SVM avec cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\multiclass.py:81: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.00917673 6.15166998 4.85522914]\n",
      "[0.43297696 0.4484303  0.32841825]\n",
      "[0.86598999 0.87977382 0.88399611]\n",
      "[0.84546102 0.86121606 0.86284829]\n",
      "[0.8475128  0.86216063 0.86600443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scoring = ['precision_samples', 'recall_samples', 'f1_samples']\n",
    "scores_svm = cross_validate(classifier_svm, vect_whole_docs, whole_labels, cv=3, scoring=scoring)\n",
    "print(scores_svm['fit_time'])\n",
    "print(scores_svm['score_time'])\n",
    "print(scores_svm['test_precision_samples'])\n",
    "print(scores_svm['test_recall_samples'])\n",
    "print(scores_svm['test_f1_samples'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.\tExercice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Réaliser le même processus avec le KNN et comparer les performances obtenues avec SVM.\n",
    "\n",
    "Q2. Réaliser le même processus avec une méthode ensembliste et comparer les performances obtenues avec KNN et SVM. \n",
    "\n",
    "NB: There is no multi-label version of AdaBoost and the label should be the shape of samples number. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(vect_train_docs,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81       719\n",
      "           1       1.00      0.35      0.52        23\n",
      "           2       1.00      0.36      0.53        14\n",
      "           3       0.55      0.60      0.57        30\n",
      "           4       0.69      0.50      0.58        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.94      0.89      0.91        18\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.88      0.82      0.85        28\n",
      "          10       0.82      0.78      0.80        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.77      0.61      0.68        56\n",
      "          13       1.00      0.45      0.62        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.79      0.54      0.64        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.83      0.90      0.87       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.76      0.59      0.67        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.91      0.97      0.94      1087\n",
      "          22       0.00      0.00      0.00        10\n",
      "          23       1.00      0.41      0.58        17\n",
      "          24       0.93      0.71      0.81        35\n",
      "          25       0.91      0.67      0.77        30\n",
      "          26       0.88      0.80      0.84       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.50      0.67         6\n",
      "          31       1.00      0.75      0.86         4\n",
      "          32       0.00      0.00      0.00         7\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.82      0.66      0.73       131\n",
      "          35       0.91      0.83      0.87        12\n",
      "          36       0.90      0.64      0.75        14\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       1.00      0.48      0.65        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.00      0.00      0.00        14\n",
      "          41       1.00      0.33      0.50         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.68      0.62      0.65        24\n",
      "          44       0.00      0.00      0.00         6\n",
      "          45       0.00      0.00      0.00        19\n",
      "          46       0.71      0.87      0.78       179\n",
      "          47       0.68      0.76      0.72        34\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.83      0.50      0.62        30\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.00      0.00      0.00         6\n",
      "          54       0.74      0.30      0.42        47\n",
      "          55       1.00      0.45      0.62        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.60      0.75        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       1.00      0.08      0.15        12\n",
      "          60       0.00      0.00      0.00         7\n",
      "          61       0.00      0.00      0.00         3\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       1.00      0.44      0.62         9\n",
      "          66       0.62      0.28      0.38        18\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       1.00      0.04      0.08        24\n",
      "          69       0.90      0.75      0.82        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.79      0.70      0.74        89\n",
      "          72       1.00      0.12      0.22         8\n",
      "          73       1.00      0.10      0.18        10\n",
      "          74       0.00      0.00      0.00        13\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.89      0.24      0.38        33\n",
      "          77       0.00      0.00      0.00        11\n",
      "          78       0.97      0.81      0.88        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         2\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       0.00      0.00      0.00         4\n",
      "          83       1.00      0.50      0.67        12\n",
      "          84       0.75      0.68      0.71       117\n",
      "          85       0.90      0.49      0.63        37\n",
      "          86       0.75      0.75      0.75        71\n",
      "          87       1.00      0.50      0.67        10\n",
      "          88       1.00      0.14      0.25        14\n",
      "          89       1.00      0.08      0.14        13\n",
      "\n",
      "   micro avg       0.87      0.73      0.79      3744\n",
      "   macro avg       0.52      0.31      0.36      3744\n",
      "weighted avg       0.84      0.73      0.76      3744\n",
      " samples avg       0.80      0.79      0.79      3744\n",
      "\n",
      "0.7316992381583306\n"
     ]
    }
   ],
   "source": [
    "test_labels_predict=knn.predict(vect_test_docs)\n",
    "print(classification_report(test_labels,test_labels_predict))\n",
    "scores=knn.score(vect_test_docs,test_labels)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7316992381583306\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous remarquons que le svm est bien meilleur que le  knn ceci revient au fait que des colonnes peuvent etre correler entre eux ce qui va diminuer la performance du knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "bc = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "bc.fit(vect_train_docs,train_labels)\n",
    "#boostPreds = boostClassifier.predict(vect_test_docs)\n",
    "scores=bc.score(vect_test_docs,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7419675389201722"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       719\n",
      "           1       0.92      0.52      0.67        23\n",
      "           2       0.85      0.79      0.81        14\n",
      "           3       0.61      0.47      0.53        30\n",
      "           4       0.68      0.72      0.70        18\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       1.00      1.00      1.00        18\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       0.00      0.00      0.00         3\n",
      "           9       0.90      0.93      0.91        28\n",
      "          10       1.00      0.78      0.88        18\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.88      0.89      0.88        56\n",
      "          13       0.88      0.75      0.81        20\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.60      0.54      0.57        28\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.88      0.84      0.86       189\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.58      0.80      0.67        44\n",
      "          20       0.00      0.00      0.00         4\n",
      "          21       0.98      0.97      0.97      1087\n",
      "          22       0.67      0.20      0.31        10\n",
      "          23       0.64      0.53      0.58        17\n",
      "          24       0.78      0.60      0.68        35\n",
      "          25       0.84      0.70      0.76        30\n",
      "          26       0.93      0.90      0.91       149\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.57      0.80      0.67         5\n",
      "          30       1.00      0.50      0.67         6\n",
      "          31       0.60      0.75      0.67         4\n",
      "          32       0.50      0.14      0.22         7\n",
      "          33       0.33      1.00      0.50         1\n",
      "          34       0.85      0.57      0.68       131\n",
      "          35       0.42      0.67      0.52        12\n",
      "          36       0.70      0.50      0.58        14\n",
      "          37       1.00      1.00      1.00         1\n",
      "          38       0.75      0.71      0.73        21\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.62      0.57      0.59        14\n",
      "          41       0.75      1.00      0.86         3\n",
      "          42       0.00      0.00      0.00         1\n",
      "          43       0.48      0.50      0.49        24\n",
      "          44       0.50      0.33      0.40         6\n",
      "          45       0.89      0.42      0.57        19\n",
      "          46       0.77      0.63      0.70       179\n",
      "          47       0.83      0.74      0.78        34\n",
      "          48       0.50      0.25      0.33         4\n",
      "          49       0.57      0.40      0.47        30\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       1.00      0.50      0.67         2\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.50      0.17      0.25         6\n",
      "          54       0.68      0.87      0.77        47\n",
      "          55       1.00      0.91      0.95        11\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.90      0.95        10\n",
      "          58       0.00      0.00      0.00         1\n",
      "          59       0.25      0.08      0.12        12\n",
      "          60       0.80      0.57      0.67         7\n",
      "          61       1.00      0.67      0.80         3\n",
      "          62       0.33      0.33      0.33         3\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       1.00      0.33      0.50         3\n",
      "          65       0.73      0.89      0.80         9\n",
      "          66       0.69      0.50      0.58        18\n",
      "          67       0.33      0.50      0.40         2\n",
      "          68       0.83      0.62      0.71        24\n",
      "          69       0.86      1.00      0.92        12\n",
      "          70       0.00      0.00      0.00         1\n",
      "          71       0.81      0.65      0.72        89\n",
      "          72       0.86      0.75      0.80         8\n",
      "          73       0.60      0.90      0.72        10\n",
      "          74       0.33      0.23      0.27        13\n",
      "          75       0.67      0.18      0.29        11\n",
      "          76       0.70      0.70      0.70        33\n",
      "          77       0.29      0.18      0.22        11\n",
      "          78       0.91      0.86      0.89        36\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.33      0.50      0.40         2\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       0.00      0.00      0.00         4\n",
      "          83       0.92      0.92      0.92        12\n",
      "          84       0.75      0.74      0.74       117\n",
      "          85       0.72      0.57      0.64        37\n",
      "          86       0.86      0.87      0.87        71\n",
      "          87       0.67      0.60      0.63        10\n",
      "          88       0.40      0.14      0.21        14\n",
      "          89       0.91      0.77      0.83        13\n",
      "\n",
      "   micro avg       0.88      0.80      0.84      3744\n",
      "   macro avg       0.59      0.51      0.53      3744\n",
      "weighted avg       0.87      0.80      0.83      3744\n",
      " samples avg       0.83      0.84      0.83      3744\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ismail\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels,bc.predict(vect_test_docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons que pour la macro qui traite le scoring de chaque classe individuellement des autres la méthode de gradientboosting est bien meilleur que le svm mais en macro avg et les autres  SVM depasse cette methode  la SVM reste un des modeles les plus performant du fait qu'elle maximise les marges qui sépare un groupe de la frontiére ceci rend l'apprentissage plus adéquat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
