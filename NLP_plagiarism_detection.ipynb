{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier1 : Text Mining/NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tObjectif \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L’objectif de cet atelier est d’apprendre les tâches NLP les plus courantes à travers l’utilisation des bibliothèques nltk, scikitlearn et Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tOutils et environnement de travail \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Installer Anaconda\n",
    "Installer les packages nltk , Spacy, Scikitlearn et pywsd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ismail\\anaconda3\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (20.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywsd in c:\\users\\ismail\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.18.1)\n",
      "Requirement already satisfied: wn in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (0.0.22)\n",
      "Requirement already satisfied: six in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.14.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (3.4.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pandas->pywsd) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pandas->pywsd) (2.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tLes bases de la NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.\tSegmentation (Tokenization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La segmentation de texte et la tâche de subdivision du texte en petites unités qui seront plus simples à traiter et qu’on appelle tokens.\n",
    "La bibliothèque nltk offre à travers le module tekenize un certain nombre de tokinzers qui permettent de réaliser la segmentation du texte en fonction de la nature du problème : words tokenizer, regular-expression based tokenizer, sentences based tokinizers, etc. Ci-dessous une liste non exhaustive de quelques fonctions du module tokinize. \n",
    "\n",
    "-regexp_span_tokenize(text, regexp: Retourne les tokens de texte qui correspondent à l’expression régulière regexp\n",
    "-sent_tokenize(text[, language]):\tRetourn les phrases contenues dans le texte en utilisant le tokenizer PunktSentenceTokenizer.\n",
    "-word_tokenize(text[, language]:\tRetourn les mots contenus dans le texte en utilisant le tokenizer TreebankWordTokenizer avec PunktSentenceTokenizer. \n",
    "\n",
    "nltk offre également un certain nombre de classes qui offrent des tokinizers plus avancés : BlanklineTokenizer, MWETokenizer, PunktSentenceTokenizer, TextTilingTokenizer, TweetTokenizer, etc. \n",
    "Ci-dessous deux exemples de tokenization à base de sent_tokenize et word_tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, i am very happy to meet you.', 'I created this course for you.', 'Good by!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "\n",
    "sentences=sent_tokenize(data) #divise en phrase\n",
    "print(sentences)\n",
    "words=word_tokenize(data)#divise en word token\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.\tNettoyage(Cleaning) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le nettoyage des données texte joue un rôle très important dans l’amélioration des performances des opérations d’analyse et de découverte de paternes. Ça consiste à la suppression des termes non significatifs \"Stop words\", comme par exemple « le », « la », « de », « du », « ce »… en français et « as » « the », « a », « an », « in » en anglais.  Ces termer qui sont présents fréquemment dans des documents texte peuvent influencer négativement sur la qualité des résultats d’analyse. \n",
    "Le nettoyage peut consister également à la supression des caratères de pontuation et des chaînes de caractères non alphabétiques. \n",
    "Ci-dessous le code qui permet de supprimer les stop words à partir d’un texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'happy', 'meet', 'created', 'course', 'good']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "data_clean = [word for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(data_clean)\n",
    "#un mot vide (ou stop word, en anglais) est un mot qui est tellement commun qu'il est inutile de l'indexer ou de l'utiliser dans une recherche. En français, des mots vides évidents pourraient être « le », « la », « de », « du », « ce »…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.\tRacinisation(Stemming)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La racinisation permet de normaliser la représentation des mots contenus dans une expression texte en extrayant leurs racines. Ça permettra de supprimer toutes les redondances des mots ayant la même racine. Plusieurs stemmers sont offerts par nltk dont les plus utilisés sont : PorterStemmer, LancasterStemmer et SnowballStemmer. Également, le module nltk.stem.snowball offre un certain nombre de stemmers personnalisés à chaque langue, comme par exemple :  FrenchStemmer, ArabicStemmer, etc. \n",
    "\n",
    "\n",
    "\n",
    "#La racine d'un mot correspond à la partie du mot restante une fois que l'on a supprimé son (ses) préfixe(s) et suffixe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'gross', 'donné', 'anglai', 'mégadonné', 'donné', 'massiv', 'désign', 'ressourc', 'inform', 'dont', 'caractéristiqu', 'term', 'volum', 'vélocité', 'variété', 'impos', 'utilis', 'technolog', 'méthode', 'analytiqu', 'particulièr', 'générer', 'valeur', 'dépassent', 'général', 'capacité', 'seul', 'uniqu', 'machin', 'nécessit', 'traitement', 'parallélisé']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#stemmer=SnowballStemmer('french')\n",
    "stemmer=PorterStemmer()\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "for i in range(len(word_tokens)):\n",
    "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('french')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analytiqu',\n",
       " 'anglai',\n",
       " 'big',\n",
       " 'capacité',\n",
       " 'caractéristiqu',\n",
       " 'data',\n",
       " 'donné',\n",
       " 'dont',\n",
       " 'dépassent',\n",
       " 'désign',\n",
       " 'gross',\n",
       " 'général',\n",
       " 'générer',\n",
       " 'impos',\n",
       " 'inform',\n",
       " 'machin',\n",
       " 'massiv',\n",
       " 'mégadonné',\n",
       " 'méthode',\n",
       " 'nécessit',\n",
       " 'parallélisé',\n",
       " 'particulièr',\n",
       " 'ressourc',\n",
       " 'seul',\n",
       " 'technolog',\n",
       " 'term',\n",
       " 'traitement',\n",
       " 'uniqu',\n",
       " 'utilis',\n",
       " 'valeur',\n",
       " 'variété',\n",
       " 'volum',\n",
       " 'vélocité'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'dat', 'gross', 'don', 'anglais', 'mégadon', 'don', 'massiv', 'désign', 'ressourc', 'inform', 'dont', 'caractérist', 'term', 'volum', 'véloc', 'variet', 'imposent', 'utilis', 'technolog', 'méthod', 'analyt', 'particuli', 'géner', 'valeur', 'dep', 'général', 'capac', 'seul', 'uniqu', 'machin', 'nécessitent', 'trait', 'parallélis']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stemmer=SnowballStemmer('french')\n",
    "#stemmer=PorterStemmer()\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('french')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.\tLemmatisation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A la différence de la racisation qui fournit souvent une représentation non significative et incomplète des mots, la lemmatisation permet d’obtenir les formes canoniques des mots contenus dans une expression texte. Ainsi, au lieu de supprimer juste les suffixes et les préfixes des mots pour obtenir leurs racines, la lemmatisation réalise une analyse morphologique des mots afin d’extraire leurs formats canoniques.\n",
    "nltk offre le lemmatizer  WordNetLemmatizer pour la réalisation des opérations de lemmatisation, mais uniquement pour l’anglais. pour d'autre langues voir Spacy dans la section 3.8.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'field', 'treat', 'way', 'analyze', 'systematically', 'extract', 'information', 'otherwise', 'deal', 'data', 'set', 'large', 'complex', 'dealt', 'traditional', 'application', 'software', 'data', 'many', 'case', 'row', 'offer', 'greater', 'statistical', 'power', 'data', 'higher', 'complexity', 'attribute', 'column', 'may', 'lead', 'higher', 'false', 'discovery', 'rate']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmmatizer=WordNetLemmatizer()\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "words = word_tokenize(data)\n",
    "words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.\tPOS-Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le pos-tagging permet des réaliser une analyse lexicale d’une expression texte selon les règles de la grammaire. Les différentes unités seront dotées d’une annotation permettant de savoir le rôle grammatical de chaque mot dans l’expression. Les annotations les plus courante sont (DT : Determiner, NN : noun , JJ : adjective,  RB: adverb, VB : verb,  PRP : Personal Pronoun…).\n",
    "\n",
    "NLTK offre une panoplie de taggers pour le pos-taggin qui recoivent une liste de tokens et leurs attribuent automatiquement  des tags en se basant sur des corpus d'apprentisgae.  \n",
    "\n",
    "par defaut la methode pos_tag offre un pos_tagging standard (Recommendé) pour l'anglais et cela en se bsant sur le tagset \"Penn Treebank\":c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Big', 'NNP'), ('data', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('that', 'WDT'), ('treats', 'VBZ'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'VB'), (',', ','), ('systematically', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN'), ('.', '.'), ('Data', 'NNP'), ('with', 'IN'), ('many', 'JJ'), ('cases', 'NNS'), ('(', '('), ('rows', 'NNS'), (')', ')'), ('offer', 'VBP'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'IN'), ('data', 'NNS'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'RBR'), ('attributes', 'NNS'), ('or', 'CC'), ('columns', 'NNS'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'DT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "words=word_tokenize(data)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dans le cas d'un document qui se compose de plusieurs phrases, il sera preferable d'utliser pos_tag_sents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Big', 'NNP'), ('data', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('that', 'WDT'), ('treats', 'VBZ'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'VB'), (',', ','), ('systematically', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN'), ('.', '.')], [('Data', 'NNP'), ('with', 'IN'), ('many', 'JJ'), ('cases', 'NNS'), ('(', '('), ('rows', 'NNS'), (')', ')'), ('offer', 'VBP'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'IN'), ('data', 'NNS'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'RBR'), ('attributes', 'NNS'), ('or', 'CC'), ('columns', 'NNS'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'DT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "\n",
    "\n",
    "sentences=sent_tokenize(data)\n",
    "\n",
    "list=[]\n",
    "for sentence in sentences:\n",
    "    list.append(word_tokenize(sentence))\n",
    "    \n",
    "print(nltk.pos_tag_sents(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big', 'NNP'),\n",
       " ('data', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('field', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('treats', 'VBZ'),\n",
       " ('ways', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', 'VB'),\n",
       " (',', ','),\n",
       " ('systematically', 'RB'),\n",
       " ('extract', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('from', 'IN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('otherwise', 'RB'),\n",
       " ('deal', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('sets', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('too', 'RB'),\n",
       " ('large', 'JJ'),\n",
       " ('or', 'CC'),\n",
       " ('complex', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('dealt', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('by', 'IN'),\n",
       " ('traditional', 'JJ'),\n",
       " ('data-processing', 'JJ'),\n",
       " ('application', 'NN'),\n",
       " ('software', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag_sents(list)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "UnigramTagger permet d'attribuer aux mots leurs tags les plus frequents par rapport à un corpus d'apprentissage.    finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.corpus Ensemble fini de textes choisi comme base d'une étude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ismail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121200039868434\n",
      "[('Big', 'JJ-TL'), ('data', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('field', 'NN'), ('that', 'CS'), ('treats', None), ('ways', 'NNS'), ('to', 'TO'), ('analyze', None), (',', ','), ('systematically', None), ('extract', None), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'VB'), ('with', 'IN'), ('data', 'NN'), ('sets', 'VBZ-HL'), ('that', 'CS'), ('are', 'BER'), ('too', 'QL'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'BE'), ('dealt', 'VBD'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', None), ('application', 'NN'), ('software', None), ('.', '.'), ('Data', None), ('with', 'IN'), ('many', 'AP'), ('cases', 'NNS'), ('(', '('), ('rows', None), (')', ')'), ('offer', 'VB'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'CS'), ('data', 'NN'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'AP'), ('attributes', None), ('or', 'CC'), ('columns', None), (')', ')'), ('may', 'MD'), ('lead', 'NN'), ('to', 'TO'), ('a', 'AT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', None), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown #The Brown Corpus was the first million-word electronic corpus of English\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n",
    "\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(unigram_tagger.tag(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "le modèle n-gram est une generalisation de l'unigram qui cnsidère également le contexte où apparait le mot en considerant les tags des n-1 mots precedents.\n",
    "\n",
    "bigram tagger est un exemple generateur pos-tagging n-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10206319146815508\n",
      "[('big', None), ('data', None), ('is', None), ('a', None), ('field', None), ('that', None), ('treats', None), ('ways', None), ('to', None), ('analyze', None), (',', None), ('systematically', None), ('extract', None), ('information', None), ('from', None), (',', None), ('or', None), ('otherwise', None), ('deal', None), ('with', None), ('data', None), ('sets', None), ('that', None), ('are', None), ('too', None), ('large', None), ('or', None), ('complex', None), ('to', None), ('be', None), ('dealt', None), ('with', None), ('by', None), ('traditional', None), ('data-processing', None), ('application', None), ('software', None), ('.', None), ('data', None), ('with', None), ('many', None), ('cases', None), ('(', None), ('rows', None), (')', None), ('offer', None), ('greater', None), ('statistical', None), ('power', None), (',', None), ('while', None), ('data', None), ('with', None), ('higher', None), ('complexity', None), ('(', None), ('more', None), ('attributes', None), ('or', None), ('columns', None), (')', None), ('may', None), ('lead', None), ('to', None), ('a', None), ('higher', None), ('false', None), ('discovery', None), ('rate', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print(bigram_tagger.evaluate(test_sents))\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(bigram_tagger.tag(word_tokenize(data.lower()))) #bigram= on traite deux termes par deux"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "combining taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452108043456593\n",
      "[('Big', 'JJ-TL'), ('data', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('field', 'NN'), ('that', 'CS'), ('treats', 'NN'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'NN'), (',', ','), ('systematically', 'NN'), ('extract', 'NN'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'VB'), ('with', 'IN'), ('data', 'NN'), ('sets', 'VBZ-HL'), ('that', 'CS'), ('are', 'BER'), ('too', 'QL'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'NN'), ('to', 'TO'), ('be', 'BE'), ('dealt', 'VBD'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'NN'), ('application', 'NN'), ('software', 'NN'), ('.', '.'), ('Data', 'NN'), ('with', 'IN'), ('many', 'AP'), ('cases', 'NNS'), ('(', '('), ('rows', 'NN'), (')', ')'), ('offer', 'VB'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'CS'), ('data', 'NN'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'AP'), ('attributes', 'NN'), ('or', 'CC'), ('columns', 'NN'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'AT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print(t2.evaluate(test_sents))\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(t2.tag(word_tokenize(data)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour le moment la package nltk ne permet de faire le pos-tagging que pour l’anglais et le russe à l’aide du modèle « averaged_perceptron_tagger ». \n",
    "StanfordPOSTagguer permet faire du pos-tagging pour d’autre langues comme le français et l’arabe. Il suffit de télécharger les differents librairies nécessaires (https://nlp.stanford.edu/software/tagger.shtml) et utiliser celles qui correspondent à la langue comme présenté dans l’exemple ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = \"C:/Users/ismail/Downloads/sqldeveloper/jdk/jre/bin\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Le', 'DET'), ('big', 'ADJ'), ('data', 'NOUN'), ('«', 'PUNCT'), ('grosses', 'ADJ'), ('données', 'NOUN'), ('»', 'PUNCT'), ('en', 'ADP'), ('anglais', 'NOUN'), (',', 'PUNCT'), ('les', 'DET'), ('mégadonnées', 'NOUN'), ('ou', 'CCONJ'), ('les', 'DET'), ('données', 'NOUN'), ('massives', 'ADJ'), (',', 'PUNCT'), ('désigne', 'VERB'), ('les', 'DET'), ('ressources', 'NOUN'), ('d', 'ADP'), ('’', 'NUM'), ('informations', 'NOUN'), ('dont', 'PRON'), ('les', 'DET'), ('caractéristiques', 'NOUN'), ('en', 'ADP'), ('termes', 'NOUN'), ('de', 'ADP'), ('volume', 'NOUN'), (',', 'PUNCT'), ('de', 'ADP'), ('vélocité', 'NOUN'), ('et', 'CCONJ'), ('de', 'ADP'), ('variété', 'NOUN'), ('imposent', 'VERB'), ('l', 'DET'), ('’', 'PUNCT'), ('utilisation', 'NOUN'), ('de', 'ADP'), ('technologies', 'NOUN'), ('et', 'CCONJ'), ('de', 'ADP'), ('méthodes', 'NOUN'), ('analytiques', 'ADJ'), ('particulières', 'ADJ'), ('pour', 'ADP'), ('générer', 'VERB'), ('de', 'ADP'), ('la', 'DET'), ('valeur', 'NOUN'), (',', 'PUNCT'), ('et', 'CCONJ'), ('qui', 'PRON'), ('dépassent', 'VERB'), ('en', 'ADP'), ('général', 'NOUN'), ('les', 'DET'), ('capacités', 'NOUN'), (\"d'une\", 'ADJ'), ('seule', 'ADJ'), ('et', 'CCONJ'), ('unique', 'ADJ'), ('machine', 'NOUN'), ('et', 'CCONJ'), ('nécessitent', 'VERB'), ('des', 'DET'), ('traitements', 'NOUN'), ('parallélisés', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "#root=\"/Users/rachad/Desktop/Labs/Labs Material/stanford-postagger-full-2020-11-17\"\n",
    "root=\"C:/Users/ismail/Desktop/stanford-postagger-full-2020-11-17\"\n",
    "stf = StanfordPOSTagger(root+'/models/french-ud.tagger',\"C:/Users/ismail/Desktop/stanford-postagger-full-2020-11-17/stanford-postagger.jar\",encoding='utf8')\n",
    "tokens = nltk.word_tokenize(data) \n",
    "print(stf.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.\tNER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La fonction nltk.ne_chunk () permet de reconnaitre les entités d’une expression texte à l'aide du modèle « maxent_ne_chunker » qui est consacré au NER pour la langue anglaise. Pour d’autre langues spaCy est recommandé. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON William/NNP)\n",
      "  (PERSON Henry/NNP Gates/NNP III/NNP)\n",
      "  (/(\n",
      "  born/JJ\n",
      "  October/NNP\n",
      "  28/CD\n",
      "  ,/,\n",
      "  1955/CD\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  business/NN\n",
      "  magnate/NN\n",
      "  ,/,\n",
      "  software/NN\n",
      "  developer/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  philanthropist/NN\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  best/RB\n",
      "  known/VBN\n",
      "  as/IN\n",
      "  the/DT\n",
      "  co-founder/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  Corporation.During/VBG\n",
      "  his/PRP$\n",
      "  career/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  ,/,\n",
      "  (PERSON Gates/NNP)\n",
      "  held/VBD\n",
      "  the/DT\n",
      "  positions/NNS\n",
      "  of/IN\n",
      "  chairman/NN\n",
      "  ,/,\n",
      "  chief/JJ\n",
      "  executive/NN\n",
      "  officer/NN\n",
      "  (/(\n",
      "  (ORGANIZATION CEO/NNP)\n",
      "  )/)\n",
      "  ,/,\n",
      "  president/NN\n",
      "  and/CC\n",
      "  chief/NN\n",
      "  software/NN\n",
      "  architect/NN\n",
      "  ,/,\n",
      "  while/IN\n",
      "  also/RB\n",
      "  being/VBG\n",
      "  the/DT\n",
      "  largest/JJS\n",
      "  individual/JJ\n",
      "  shareholder/NN\n",
      "  until/IN\n",
      "  May/NNP\n",
      "  2014/CD\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  best-known/JJ\n",
      "  entrepreneurs/NNS\n",
      "  and/CC\n",
      "  pioneers/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  microcomputer/NN\n",
      "  revolution/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  1970s/CD\n",
      "  and/CC\n",
      "  1980s/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"William Henry Gates III (born October 28, 1955) is an American business magnate,\" \\\n",
    "     \" software developer, and philanthropist. He is best known as the co-founder of Microsoft Corporation.\" \\\n",
    "     \"During his career at Microsoft, Gates held the positions of chairman, \" \\\n",
    "     \"chief executive officer (CEO), president and chief software architect, \" \\\n",
    "     \"while also being the largest individual shareholder until May 2014. \" \\\n",
    "     \"He is one of the best-known entrepreneurs and pioneers of the microcomputer \" \\\n",
    "     \"revolution of the 1970s and 1980s.\"\n",
    "words=word_tokenize(data)\n",
    "print(nltk.ne_chunk(nltk.pos_tag(words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.\tWSD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Un mot peut avoir plusieurs significations selon son contexte (les mots voisins et le rôle grammaticale). Par exemple, le mot anglais « break » possède 75 sens. Chose qui montre l’importance de la désambiguïsation lors de l’analyse d’un texte. Ci-dessous un extrait de la récupération des différents sens du mot « break » avec leurs annotations grammaticales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> some abrupt occurrence that interrupts an ongoing activity\n",
      ">>> an unexpected piece of good luck\n",
      ">>> (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n",
      ">>> a personal or social separation (as between opposing factions)\n",
      ">>> a pause from doing something (as work)\n",
      ">>> the act of breaking something\n",
      ">>> a time interval during which there is a temporary cessation of something\n",
      ">>> breaking of hard tissue such as bone\n",
      ">>> the occurrence of breaking\n",
      ">>> an abrupt change in the tone or register of the voice (as at puberty or due to emotion)\n",
      ">>> the opening shot that scatters the balls in billiards or pool\n",
      ">>> (tennis) a score consisting of winning a game when your opponent was serving\n",
      ">>> an act of delaying or interrupting the continuity\n",
      ">>> a sudden dash\n",
      ">>> any frame in which a bowler fails to make a strike or spare\n",
      ">>> an escape from jail\n",
      ">>> terminate\n",
      ">>> become separated into pieces or fragments\n",
      ">>> render inoperable or ineffective\n",
      ">>> ruin completely\n",
      ">>> destroy the integrity of; usually by force; cause to separate into pieces or fragments\n",
      ">>> act in disregard of laws, rules, contracts, or promises\n",
      ">>> move away or escape suddenly\n",
      ">>> scatter or part\n",
      ">>> force out or release suddenly and often violently something pent up\n",
      ">>> prevent completion\n",
      ">>> enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\n",
      ">>> make submissive, obedient, or useful\n",
      ">>> fail to agree with; be in violation of; as of rules or patterns\n",
      ">>> surpass in excellence\n",
      ">>> make known to the public information that was previously known only to a few people or that was meant to be kept a secret\n",
      ">>> come into being\n",
      ">>> stop operating or functioning\n",
      ">>> interrupt a continued activity\n",
      ">>> make a rupture in the ranks of the enemy or one's own by quitting or fleeing\n",
      ">>> curl over and fall apart in surf or foam, of waves\n",
      ">>> lessen in force or effect\n",
      ">>> be broken in\n",
      ">>> come to an end\n",
      ">>> vary or interrupt a uniformity or continuity\n",
      ">>> cause to give up a habit\n",
      ">>> give up\n",
      ">>> come forth or begin from a state of latency\n",
      ">>> happen or take place\n",
      ">>> cause the failure or ruin of\n",
      ">>> invalidate by judicial action\n",
      ">>> discontinue an association or relation; go different ways\n",
      ">>> assign to a lower position; reduce in rank\n",
      ">>> reduce to bankruptcy\n",
      ">>> change directions suddenly\n",
      ">>> emerge from the surface of a body of water\n",
      ">>> break down, literally or metaphorically\n",
      ">>> do a break dance\n",
      ">>> exchange for smaller units of money\n",
      ">>> destroy the completeness of a set of related items\n",
      ">>> make the opening shot that scatters the balls\n",
      ">>> separate from a clinch, in boxing\n",
      ">>> go to pieces\n",
      ">>> break a piece from a whole\n",
      ">>> become punctured or penetrated\n",
      ">>> pierce or penetrate\n",
      ">>> be released or become known; of news\n",
      ">>> cease an action temporarily\n",
      ">>> interrupt the flow of current in\n",
      ">>> undergo breaking\n",
      ">>> find a flaw in\n",
      ">>> find the solution or key to\n",
      ">>> change suddenly from one tone quality or register to another\n",
      ">>> happen\n",
      ">>> become fractured; break or crack on the surface only\n",
      ">>> crack; of the male voice in puberty\n",
      ">>> fall sharply\n",
      ">>> fracture a bone of\n",
      ">>> diminish or discontinue abruptly\n",
      ">>> weaken or destroy in spirit or body\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for sens in wordnet.synsets('break'):\n",
    "    print(\">>>\",sens.definition())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La bibliothèque nltk offre à travers le module wsd la possibilité de détecter le sens d’un mot en fonction de son contexte. A cette fin, l’algorithme Lesk est utilisé pour réaliser une désambiguïsation du sens d’un mot en retournant le sens qui a permis d’avoir le plus grand nombre de termes en intersection avec le contexte du mot pour lequel on est en train de chercher le sens exact. L’algorithme ne retourne aucun sens s’il n’arrive pas à réaliser la désambiguïsation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "context= word_tokenize(\"I've just finished the first step of the competition. I need a little break to catch my breath\")\n",
    "print(lesk(context, 'break','n').definition())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L’exemple ci-dessus montre que l’algorithme n’est pas assez performant. D’autres algorithmes peuvent être utilisés en se basant sur les bibliothèques baseline, pywsd ou spaCy. Ci-dessous un autre exemple avec la bibliotheque pywsd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywsd in c:\\users\\ismail\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: six in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.14.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.0.1)\n",
      "Requirement already satisfied: wn in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (0.0.22)\n",
      "Requirement already satisfied: nltk in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (3.4.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pywsd) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pandas->pywsd) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pandas->pywsd) (2.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wn==0.0.22 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (0.0.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wn==0.0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n",
      "a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "took 4.580479621887207 secs.\n"
     ]
    }
   ],
   "source": [
    "# pip install pywsd\n",
    "# pip install -U pywsd\n",
    "#pip install wn==0.0.22\n",
    "from pywsd.lesk import simple_lesk\n",
    "sent = 'I went to the bank to deposit my money'\n",
    "ambiguous = 'bank'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print (answer)\n",
    "print (answer.definition())\n",
    "#It is based on the hypothesis that words used together \n",
    "#in text are related to each other and that the relation \n",
    "#can be observed in the definitions of the words and their senses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8.\tSpacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spaCy une bibliothèque de la NLP qui est très puissante (elle est orientée production, non uniquement pour la recherche ou l’apprentissage de la NLP), totalement écrite python, gratuite et libre.\n",
    "Par défaut, lorsqu’on fait appel au module nlp de spaCy, les opérations suivantes sont exécutées : Segmentation, pos-tagging, analyse syntaxique, NER (Named Entity Recognition), etc.  Un objet Doc est retourné à l’issue de toutes les opérations et qui encapsule tous les resultats de l’analyse. \n",
    "\n",
    "L’exemple ci-dessous montre comment extraire les différentes informations à partir d’un objet Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ismail\\anaconda3\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (20.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.14.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ismail\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "/token: Le /lemma: le Xx True True /POS: DET /PARS: big det /NER: O \n",
      "/token: big /lemma: big xxx True False /POS: NUM /PARS: data nummod /NER: O \n",
      "/token: data /lemma: data xxxx True False /POS: ADP /PARS: data ROOT /NER: O \n",
      "/token:   /lemma:     False False /POS: SPACE /PARS: data punct /NER: O \n",
      "/token: « /lemma: « « False False /POS: NOUN /PARS: « ROOT /NER: O \n",
      "/token: grosses /lemma: gros xxxx True False /POS: ADJ /PARS: données amod /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: données ROOT /NER: O \n",
      "/token: » /lemma: » » False False /POS: PUNCT /PARS: données punct /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: anglais case /NER: O \n",
      "/token: anglais /lemma: anglais xxxx True False /POS: NOUN /PARS: données obl:mod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: mégadonnées det /NER: O \n",
      "/token: mégadonnées /lemma: mégadonnée xxxx True False /POS: NOUN /PARS: désigne nsubj /NER: O \n",
      "/token: ou /lemma: ou xx True True /POS: CCONJ /PARS: données cc /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: données det /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: mégadonnées conj /NER: O \n",
      "/token: massives /lemma: massif xxxx True False /POS: ADJ /PARS: données amod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: désigne /lemma: désigne xxxx True False /POS: VERB /PARS: désigne ROOT /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: ressources det /NER: O \n",
      "/token: ressources /lemma: ressource xxxx True False /POS: NOUN /PARS: désigne obj /NER: O \n",
      "/token: d’ /lemma: d’ x’ False True /POS: NOUN /PARS: ressources acl /NER: B PER\n",
      "/token: informations /lemma: information xxxx True False /POS: NOUN /PARS: ressources nmod /NER: O \n",
      "/token: dont /lemma: dont xxxx True True /POS: PRON /PARS: caractéristiques nmod /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: caractéristiques det /NER: O \n",
      "/token: caractéristiques /lemma: caractéristique xxxx True False /POS: NOUN /PARS: informations acl:relcl /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: termes case /NER: O \n",
      "/token: termes /lemma: terme xxxx True False /POS: NOUN /PARS: caractéristiques nmod /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: volume case /NER: O \n",
      "/token: volume /lemma: volume xxxx True False /POS: NOUN /PARS: termes nmod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: vélocité punct /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: vélocité case /NER: O \n",
      "/token: vélocité /lemma: vélocité xxxx True False /POS: NOUN /PARS: termes conj /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: variété cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: variété case /NER: O \n",
      "/token: variété /lemma: variété xxxx True False /POS: NOUN /PARS: termes conj /NER: O \n",
      "/token: imposent /lemma: imposent xxxx True False /POS: ADV /PARS: variété advmod /NER: O \n",
      "/token: l’ /lemma: l’ x’ False True /POS: ADJ /PARS: utilisation case /NER: O \n",
      "/token: utilisation /lemma: utilisation xxxx True False /POS: NOUN /PARS: variété obj /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: technologies case /NER: O \n",
      "/token: technologies /lemma: technologie xxxx True False /POS: NOUN /PARS: utilisation nmod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: méthodes cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: méthodes case /NER: O \n",
      "/token: méthodes /lemma: méthode xxxx True False /POS: NOUN /PARS: technologies conj /NER: O \n",
      "/token: analytiques /lemma: analytique xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: particulières /lemma: particulier xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: pour /lemma: pour xxxx True True /POS: ADP /PARS: générer mark /NER: O \n",
      "/token: générer /lemma: générer xxxx True False /POS: VERB /PARS: variété advcl /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: valeur case /NER: O \n",
      "/token: la /lemma: le xx True True /POS: DET /PARS: valeur det /NER: O \n",
      "/token: valeur /lemma: valeur xxxx True False /POS: NOUN /PARS: générer obl:arg /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: dépassent cc /NER: O \n",
      "/token: qui /lemma: qui xxx True True /POS: PRON /PARS: dépassent nsubj /NER: O \n",
      "/token: dépassent /lemma: dépasser xxxx True False /POS: VERB /PARS: désigne conj /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: général case /NER: O \n",
      "/token: général /lemma: général xxxx True False /POS: NOUN /PARS: dépassent obl:arg /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: capacités det /NER: O \n",
      "/token: capacités /lemma: capacité xxxx True False /POS: NOUN /PARS: dépassent obj /NER: O \n",
      "/token: d' /lemma: de x' False True /POS: ADP /PARS: machine case /NER: O \n",
      "/token: une /lemma: un xxx True True /POS: DET /PARS: machine det /NER: O \n",
      "/token: seule /lemma: seul xxxx True True /POS: ADJ /PARS: machine amod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: unique cc /NER: O \n",
      "/token: unique /lemma: unique xxxx True False /POS: ADJ /PARS: seule conj /NER: O \n",
      "/token: machine /lemma: machine xxxx True False /POS: NOUN /PARS: capacités nmod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: nécessitent cc /NER: O \n",
      "/token: nécessitent /lemma: nécessiter xxxx True False /POS: VERB /PARS: dépassent conj /NER: O \n",
      "/token: des /lemma: un xxx True True /POS: DET /PARS: traitements det /NER: O \n",
      "/token: traitements /lemma: traitement xxxx True False /POS: NOUN /PARS: nécessitent obj /NER: O \n",
      "/token: parallélisés /lemma: paralléliser xxxx True False /POS: VERB /PARS: traitements acl /NER: O \n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"/token:\",token.text, \"/lemma:\",token.lemma_, token.shape_, token.is_alpha, token.is_stop,\"/POS:\", token.tag_, \"/PARS:\", token.head, token.dep_, \"/NER:\", token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tModel représentatif d’un document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Afin de simplifier l’analyse des données texte, il est recommandé d’utiliser des représentations plus consistantes qu’une simple segmentation. Il faut bien évidement réaliser des prétraitements tels que la racinisation, la lemmatisation, supprimer les redondances, supprimer les mots qui représentent le même sens. Mais c’est encore insuffisant pour obtenir un modèle représentatif qui reflète l’importance et le sens exacte de chaque mot dans une expression ou dans un document texte.\n",
    "Afin de repondre à ce besoin, plusieurs représentations vectorielles des termes contenus dans un texte sont possibles : one-hot-vector, Bag-of-words, TF-IDF et Word2vec...t. Nous utilisant scikitlearn pour réaliser ces différentes représentations vectorielles. Ça n’empêche pas que ces représentations vectorielles peuvent être obtenues en faisant du codage from scratch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. One-hot-vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et determine par la suite pour chaque document/expression la présence de chaque terme du vocabulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "freq   = CountVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "corpus = freq.fit_transform(corpus)\n",
    "#print(corpus.toarray())\n",
    "onehot = Binarizer()\n",
    "corpus = onehot.fit_transform(corpus.toarray())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.\tBeg-of-words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la représentation vectorielle Bag-of-Words, le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et calcul par la suite pour chaque document/expression le nombre d’occurrences de chaque terme du vocabulaire. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "X = vectorizer.fit_transform(corpus) #sparsy format\n",
    "print(X.toarray()) # explicit matrix format\n",
    "print(vectorizer.get_feature_names() ) #vocabulary as list of string\n",
    "vectorizer.vocabulary_.get('document') #get column index of a specific term in the vocabulary\n",
    "vectorizer.transform(['Something completely new.']).toarray()#apply the model to a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.\tTF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La représentation vectorielle Bag-of-Words considère la fréquence d’apparition des termes du vocabulaire dans chaque document du corpus séparément des autres. Cette représentation néglige l’importance du terme par rapport au corpus tout en entier.  TF-IDF (term-frequency times inverse document-frequency) est un autre modèle de représentation vectorielle des occurrences des termes d’un document en considérant également leurs occurrences dans tout le corpus. Cette approche va permettre de diminuer l’importance des termes les plus fréquents dans des documents texte tels que les stop words.\n",
    "\n",
    "tf-idf(t,d)=tf(t,d)×idf(t).\n",
    "\n",
    "idf(t)=log[(1+n)/(1+df(t))]+1\n",
    "n: la taille du corpus.\n",
    "df(t) :  le nombre de documents qui comportent le terme t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.','this is the second second Document.','And the third one.','Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names() )\n",
    "\n",
    "#NB: le vecteur tf-idf obtenu sera normalisé pour obtenir des valeurs entre 0 et 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tCas utilisations : Détection du Plagiarisme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de cet atelier est de détecter le pélagianisme à partir de wikipedia pendant la préparation des réponses à un certain nombre de questions sur des connaissances en informatique. Le dataset utilisé peut-être récupéré à partir du lien suivant :Cliquer <a href=\"https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html#Download\" target=\"_blank\">ICI</a> \n",
    "\n",
    "Pour ce faire, nous nous basant sur le calcul des similarités entre les réponses des candidats et les définitions exactes trouvées sur Wikipédia. Deux méthodes de calcul de similarité sont à utiliser, à savoir, la similarité syntaxique et la similarité sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuperer le dataset\n",
    "**Réaliser les différentes tâches de prétraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files=glob.glob('C:/Users/ismail/Desktop/corpus/*')\n",
    "f_pointers=[open(file,\"r\",encoding=\"utf8\",errors='ignore') for file in files]\n",
    "corpus=[f.read() for f in f_pointers]\n",
    "#print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "##### print(len(corpus))\n",
    "d=[]\n",
    "for i in corpus:\n",
    "    word_tokens = [word.lower() for word in word_tokenize(i)]\n",
    "    words = [lemmmatizer.lemmatize(word.lower()) for word in word_tokens if(not word in set(stopwords.words('english')) and  word.isalpha())] \n",
    "    d.append(words)\n",
    "print(len(d))\n",
    "print(type(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aura besoin que de la lemmatization aprés avoir rendu le doc sous forme de token pour pouvoir faire la similarité syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 similarité syntaxique "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la similarité syntaxique entre des vecteurs, plusieurs distances sont possibles à savoir : distance, euclidienne, Cosine Jaccard, Levenshtein, Hamming…\n",
    "\n",
    "l'exemple ci-dessous permet de calculer la similarité. semantique entre les documents en se basant sur une representation vectorielle en TFIDF avec la distance euclidienne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.05990529, 1.33904078, 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#compute similarity for first sentence with rest of the sentences\n",
    "euclidean_distances(tfidf_matrix[0:1],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realiser la même chose en se basant sur les representations OHV et BOW?\n",
    "**Comparer les performances des trois methodes en terme de temps d'execution et precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.23606798, 2.64575131, 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "freq   = CountVectorizer()\n",
    "c=[sent.lower() for sent in corpus]\n",
    "cor = freq.fit_transform(corpus)\n",
    "\n",
    "#compute similarity for first sentence with rest of the sentences\n",
    "euclidean_distances(cor[0:1],cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.64575131, 2.23606798, 2.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "euclidean_distances(X[0:1],X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à mon avis le plus précis est le tfidf car il prend en considération meme la fréquence du terme dans\n",
    "tous le corpus c'est ce qui fait de lui plus précis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe d'autre methodes(orientées caractères) pour le calcule de la similarité syntaxique entre des documents courts (des phrases):\n",
    "\n",
    "* Longest Common Sequence (LCS)\n",
    "* Set features\n",
    "* Word Order Similarity\n",
    "* n-gram sentences \n",
    "* Jaro-Winkler\n",
    "* ...\n",
    "\n",
    "Cliquer <a href=\"http://www.iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_9_ISSUE_5/IJCET_09_05_001.pdf\" target=\"_blank\">ICI</a> pour plus de details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en utilisant la distance euclidienne on a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/ismail/Desktop/corpus\\\\g0pA_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pA_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pA_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pA_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pA_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pB_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pB_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pB_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pB_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pB_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pC_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pC_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pC_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pC_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pC_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pD_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pD_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pD_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pD_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pD_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pE_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pE_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pE_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pE_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g0pE_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pA_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pA_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pA_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pA_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pA_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pB_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pB_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pB_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pB_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pB_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pD_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pD_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pD_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pD_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g1pD_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pA_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pA_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pA_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pA_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pA_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pB_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pB_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pB_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pB_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pB_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pC_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pC_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pC_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pC_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pC_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pE_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pE_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pE_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pE_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g2pE_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pA_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pA_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pA_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pA_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pA_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pB_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pB_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pB_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pB_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pB_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pC_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pC_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pC_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pC_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g3pC_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pB_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pB_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pB_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pB_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pB_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pC_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pC_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pC_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pC_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pC_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pD_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pD_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pD_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pD_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pD_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pE_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pE_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pE_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pE_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\g4pE_taske.txt', 'C:/Users/ismail/Desktop/corpus\\\\orig_taska.txt', 'C:/Users/ismail/Desktop/corpus\\\\orig_taskb.txt', 'C:/Users/ismail/Desktop/corpus\\\\orig_taskc.txt', 'C:/Users/ismail/Desktop/corpus\\\\orig_taskd.txt', 'C:/Users/ismail/Desktop/corpus\\\\orig_taske.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files=glob.glob('C:/Users/ismail/Desktop/corpus/*')\n",
    "f_pointers=[open(file,\"r\",encoding=\"utf8\",errors='ignore') for file in files]\n",
    "corpus=[f.read() for f in f_pointers]\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculer les similarités syntaxiques entre les réponses des étudiants et les définitions trouvées sur wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska=[corpus[5*i] for i in range(19)]\n",
    "taskb=[corpus[5*i+1] for i in range(19)]\n",
    "taskc=[corpus[5*i+2] for i in range(19)]\n",
    "taskd=[corpus[5*i+3] for i in range(19)]\n",
    "taske=[corpus[5*i+4] for i in range(19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####result for task a##########\n",
      "{'Groupe 0eA:': 30.854497241083024, 'Groupe 0eB:': 35.11409973215888, 'Groupe 0eC:': 20.904544960366874, 'Groupe 0eD:': 15.556349186104045, 'Groupe 0eE:': 5.196152422706632, 'Groupe 1eA:': 25.748786379167466, 'Groupe 1eB:': 26.43860813280457, 'Groupe 1eC:': 18.0, 'Groupe 1eD:': 31.906112267087632, 'Groupe 1eE:': 33.04542328371661, 'Groupe 2eA:': 18.0, 'Groupe 2eB:': 14.142135623730951, 'Groupe 2eC:': 24.24871130596428, 'Groupe 2eD:': 27.0, 'Groupe 2eE:': 20.784609690826528, 'Groupe 3eA:': 29.086079144497972, 'Groupe 3eB:': 4.358898943540674, 'Groupe 3eC:': 19.72308292331602, 'Groupe 3eD:': 29.393876913398138}\n",
      "#####result for task b##########\n",
      "{'Groupe 0eA:': 39.26830783214372, 'Groupe 0eB:': 44.04543109109048, 'Groupe 0eC:': 51.07837115648854, 'Groupe 0eD:': 56.60388679233962, 'Groupe 0eE:': 57.59340239992772, 'Groupe 1eA:': 47.085029467974216, 'Groupe 1eB:': 46.42197755374064, 'Groupe 1eC:': 43.41658669218482, 'Groupe 1eD:': 35.93048844644336, 'Groupe 1eE:': 50.269274910227224, 'Groupe 2eA:': 46.71188285650665, 'Groupe 2eB:': 46.561786907291264, 'Groupe 2eC:': 40.64480286580315, 'Groupe 2eD:': 43.12771730569565, 'Groupe 2eE:': 49.48737212663449, 'Groupe 3eA:': 49.61854492022111, 'Groupe 3eB:': 48.48711168960263, 'Groupe 3eC:': 46.87216658103186, 'Groupe 3eD:': 38.8329756778952}\n",
      "#####result for task c##########\n",
      "{'Groupe 0eA:': 8.54400374531753, 'Groupe 0eB:': 13.152946437965905, 'Groupe 0eC:': 24.919871588754223, 'Groupe 0eD:': 14.106735979665885, 'Groupe 0eE:': 25.11971337416094, 'Groupe 1eA:': 19.183326093250876, 'Groupe 1eB:': 15.165750888103101, 'Groupe 1eC:': 22.58317958127243, 'Groupe 1eD:': 7.681145747868608, 'Groupe 1eE:': 20.518284528683193, 'Groupe 2eA:': 24.979991993593593, 'Groupe 2eB:': 29.478805945967352, 'Groupe 2eC:': 19.183326093250876, 'Groupe 2eD:': 18.708286933869708, 'Groupe 2eE:': 27.055498516937366, 'Groupe 3eA:': 14.247806848775006, 'Groupe 3eB:': 26.457513110645905, 'Groupe 3eC:': 23.853720883753127, 'Groupe 3eD:': 20.639767440550294}\n",
      "#####result for task d##########\n",
      "{'Groupe 0eA:': 24.43358344574123, 'Groupe 0eB:': 18.520259177452136, 'Groupe 0eC:': 19.8997487421324, 'Groupe 0eD:': 34.785054261852174, 'Groupe 0eE:': 33.926390907374746, 'Groupe 1eA:': 20.149441679609886, 'Groupe 1eB:': 20.760539492026695, 'Groupe 1eC:': 27.184554438136374, 'Groupe 1eD:': 30.166206257996713, 'Groupe 1eE:': 10.099504938362077, 'Groupe 2eA:': 20.615528128088304, 'Groupe 2eB:': 24.61706725018234, 'Groupe 2eC:': 1.7320508075688772, 'Groupe 2eD:': 25.13961017995307, 'Groupe 2eE:': 25.357444666211933, 'Groupe 3eA:': 15.716233645501712, 'Groupe 3eB:': 5.196152422706632, 'Groupe 3eC:': 22.67156809750927, 'Groupe 3eD:': 29.512709126747414}\n",
      "#####result for task e##########\n",
      "{'Groupe 0eA:': 46.09772228646444, 'Groupe 0eB:': 32.78719262151, 'Groupe 0eC:': 42.37924020083418, 'Groupe 0eD:': 51.59457335805772, 'Groupe 0eE:': 47.686476070265456, 'Groupe 1eA:': 45.858477951192405, 'Groupe 1eB:': 33.63034344160047, 'Groupe 1eC:': 39.8622628559895, 'Groupe 1eD:': 39.57271787481876, 'Groupe 1eE:': 28.319604517012593, 'Groupe 2eA:': 38.66522985836241, 'Groupe 2eB:': 40.80441152620633, 'Groupe 2eC:': 40.29888335921977, 'Groupe 2eD:': 40.87786687193939, 'Groupe 2eE:': 47.47630988187688, 'Groupe 3eA:': 25.88435821108957, 'Groupe 3eB:': 30.397368307141328, 'Groupe 3eC:': 34.713109915419565, 'Groupe 3eD:': 40.85339643163099}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vectorizer = CountVectorizer()\n",
    "task=corpus[95:]\n",
    "#print(task)\n",
    "tasks=[]\n",
    "tasks.append(taska)\n",
    "tasks.append(taskb)\n",
    "tasks.append(taskc)\n",
    "tasks.append(taskd)\n",
    "tasks.append(taske)\n",
    "#X = vectorizer.fit_transform(d[:96])\n",
    "#on a 4 groupes et en chaque groupe groupe 5 etudiant A B C D E DONC\n",
    "tab=[]\n",
    "for k in range(5):\n",
    "    print(\"#####result for task \"+chr(ord('a')+k)+\"##########\")\n",
    "    l=-1\n",
    "    b=0\n",
    "    tp={}\n",
    "    for t in range(19):\n",
    "        if t%5==0:\n",
    "            b=0\n",
    "            l+=1\n",
    "        temp=[]\n",
    "        temp.append(tasks[k][t])\n",
    "        temp.append(task[k])\n",
    "                #print(temp)\n",
    "        X = vectorizer.fit_transform(temp)\n",
    "        tp[\"Groupe \"+str(l)+\"e\"+chr(ord('A')+b)+\":\"]=(euclidean_distances(X[0:1],X)[0,1])\n",
    "        b+=1\n",
    "    tab.append(tp)\n",
    "    print(tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en utilisant la méthode jaccard on a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "#jaccard_similarity(sent_tokenize(corpus[0]),sent_tokenize(corpus[95]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2): \n",
    "\n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = set(doc1).intersection(set(doc2))\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = set(doc1).union(set(doc2))\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####result for task a##########\n",
      "{'Groupe 0eA:': 0.0963855421686747, 'Groupe 0eB:': 0.05759162303664921, 'Groupe 0eC:': 0.07526881720430108, 'Groupe 0eD:': 0.042328042328042326, 'Groupe 0eE:': 0.06818181818181818, 'Groupe 1eA:': 0.12352941176470589, 'Groupe 1eB:': 0.055248618784530384, 'Groupe 1eC:': 0.05963302752293578, 'Groupe 1eD:': 0.08670520231213873, 'Groupe 1eE:': 0.05851063829787234, 'Groupe 2eA:': 0.22641509433962265, 'Groupe 2eB:': 0.04, 'Groupe 2eC:': 0.022727272727272728, 'Groupe 2eD:': 0.03333333333333333, 'Groupe 2eE:': 0.07514450867052024, 'Groupe 3eA:': 0.5564516129032258, 'Groupe 3eB:': 0.019230769230769232, 'Groupe 3eC:': 0.06321839080459771, 'Groupe 3eD:': 0.02142857142857143}\n",
      "#####result for task b##########\n",
      "{'Groupe 0eA:': 0.019230769230769232, 'Groupe 0eB:': 0.08461538461538462, 'Groupe 0eC:': 0.19889502762430938, 'Groupe 0eD:': 0.020942408376963352, 'Groupe 0eE:': 0.026455026455026454, 'Groupe 1eA:': 0.01932367149758454, 'Groupe 1eB:': 0.01809954751131222, 'Groupe 1eC:': 0.14150943396226415, 'Groupe 1eD:': 0.07239819004524888, 'Groupe 1eE:': 0.03278688524590164, 'Groupe 2eA:': 0.05752212389380531, 'Groupe 2eB:': 0.03636363636363636, 'Groupe 2eC:': 0.0779816513761468, 'Groupe 2eD:': 0.08189655172413793, 'Groupe 2eE:': 0.05045871559633028, 'Groupe 3eA:': 0.04961832061068702, 'Groupe 3eB:': 0.044897959183673466, 'Groupe 3eC:': 0.20833333333333334, 'Groupe 3eD:': 0.041025641025641026}\n",
      "#####result for task c##########\n",
      "{'Groupe 0eA:': 0.03875968992248062, 'Groupe 0eB:': 0.041666666666666664, 'Groupe 0eC:': 0.06470588235294118, 'Groupe 0eD:': 0.06321839080459771, 'Groupe 0eE:': 0.7475728155339806, 'Groupe 1eA:': 0.050314465408805034, 'Groupe 1eB:': 0.04294478527607362, 'Groupe 1eC:': 0.0472972972972973, 'Groupe 1eD:': 0.06369426751592357, 'Groupe 1eE:': 0.328, 'Groupe 2eA:': 0.05454545454545454, 'Groupe 2eB:': 0.05847953216374269, 'Groupe 2eC:': 0.043209876543209874, 'Groupe 2eD:': 0.055944055944055944, 'Groupe 2eE:': 0.12857142857142856, 'Groupe 3eA:': 0.02158273381294964, 'Groupe 3eB:': 0.03333333333333333, 'Groupe 3eC:': 0.052884615384615384, 'Groupe 3eD:': 0.06338028169014084}\n",
      "#####result for task d##########\n",
      "{'Groupe 0eA:': 0.05185185185185185, 'Groupe 0eB:': 0.11764705882352941, 'Groupe 0eC:': 0.046052631578947366, 'Groupe 0eD:': 0.04225352112676056, 'Groupe 0eE:': 0.059602649006622516, 'Groupe 1eA:': 0.06315789473684211, 'Groupe 1eB:': 0.9782608695652174, 'Groupe 1eC:': 0.06962025316455696, 'Groupe 1eD:': 0.0392156862745098, 'Groupe 1eE:': 0.026595744680851064, 'Groupe 2eA:': 0.05263157894736842, 'Groupe 2eB:': 0.23140495867768596, 'Groupe 2eC:': 0.03731343283582089, 'Groupe 2eD:': 0.05263157894736842, 'Groupe 2eE:': 0.025974025974025976, 'Groupe 3eA:': 0.03305785123966942, 'Groupe 3eB:': 0.23469387755102042, 'Groupe 3eC:': 0.046153846153846156, 'Groupe 3eD:': 0.02857142857142857}\n",
      "#####result for task e##########\n",
      "{'Groupe 0eA:': 0.05286343612334802, 'Groupe 0eB:': 0.08056872037914692, 'Groupe 0eC:': 0.04326923076923077, 'Groupe 0eD:': 0.6050955414012739, 'Groupe 0eE:': 0.09795918367346938, 'Groupe 1eA:': 0.07216494845360824, 'Groupe 1eB:': 0.037037037037037035, 'Groupe 1eC:': 0.07555555555555556, 'Groupe 1eD:': 0.5632911392405063, 'Groupe 1eE:': 0.06422018348623854, 'Groupe 2eA:': 0.0502283105022831, 'Groupe 2eB:': 0.0502283105022831, 'Groupe 2eC:': 0.05652173913043478, 'Groupe 2eD:': 0.33519553072625696, 'Groupe 2eE:': 0.058823529411764705, 'Groupe 3eA:': 0.05829596412556054, 'Groupe 3eB:': 0.03414634146341464, 'Groupe 3eC:': 0.029166666666666667, 'Groupe 3eD:': 0.1542056074766355}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#vectorizer = CountVectorizer()\n",
    "task=d[95:]\n",
    "#print(task)\n",
    "tasks=[]\n",
    "tasks.append(d[:19])\n",
    "tasks.append(d[19:38])\n",
    "tasks.append(d[38:(38+19)])\n",
    "tasks.append(d[(38+19):(38+2*19)])\n",
    "tasks.append(d[(38+2*19):(38+3*19)])\n",
    "#X = vectorizer.fit_transform(d[:96])\n",
    "#on a 4 groupes et en chaque groupe groupe 5 etudiant A B C D E DONC\n",
    "tab=[]\n",
    "for k in range(5):\n",
    "    print(\"#####result for task \"+chr(ord('a')+k)+\"##########\")\n",
    "    l=-1\n",
    "    b=0\n",
    "    tp={}\n",
    "    for t in range(19):\n",
    "        if t%5==0:\n",
    "            b=0\n",
    "            l+=1\n",
    "        s1=tasks[k][t]\n",
    "        s2=task[k]\n",
    "        tp[\"Groupe \"+str(l)+\"e\"+chr(ord('A')+b)+\":\"]=jaccard_similarity(s1,s2)\n",
    "        b+=1\n",
    "    tab.append(tp)\n",
    "    print(tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 similarité semantique : Approche lexicale"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WordNet est une base de données lexicale qui compoerte des concepts (termes) classifiés et reliés les uns aux autres à travers des realtions semantiques\n",
    "\n",
    "La composante principale de wordNet est le synset (synonym set) tel que chacun contient plusieurs mots qui partagent le même sens (des lemmas). Egalement, un mot peut appartenir à plusieurs synsets à la foix.\n",
    "\n",
    "l'exemple suivant montre comment recuperer les synsets d'aun mots et comment recuperer ses synonymes pour un sens particuliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer sens in wordNet:\n",
      " \t Sens : 0\n",
      " \t\t Sens definition: a machine for performing calculations automatically\n",
      "\t\t Lemmas for sense :['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      " \t Sens : 1\n",
      " \t\t Sens definition: an expert at calculation (or at operating calculating machines)\n",
      "\t\t Lemmas for sense :['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "computer_synsets = wn.synsets(\"computer\") \n",
    "print(\"Computer sens in wordNet:\")\n",
    "i=0;\n",
    "for sense in computer_synsets: \n",
    "    print(\" \\t Sens :\", i)\n",
    "    print(\" \\t\\t Sens definition: \"+sense.definition())\n",
    "    lemmas = [l.name() for l in sense.lemmas()]\n",
    "    print(\"\\t\\t Lemmas for sense :\" +str(lemmas))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la similarité sémantique, la bibliothèque nltk et à travers le module wordnet permet de mesurer la distance ou la similarité sémantique entre les sens des mots. Ainsi, en récupérant les sens synset1 et synset2 de deux mots quelconques plusieurs façons sont possibles pour calculer leur similarité\n",
    "•\tsynset1.path_similarity(synset2) : retourne leur ordre de similarité sous forme d’une valeur numérique entre 0 et 1 en se basant sur le plus court chemin qui relie les deux sens dans l’arborescence de wordnet.\n",
    "•\tsynset1.lch_similarity(synset2): qui se base sur l’algorithme Leacock-Chodorow\n",
    "•\tSynset1.wup_similarity(synset2): qui se base sur l’algorithme Wu-Palmer\n",
    "•\tsynset1.res_similarity(synset2, ic): qui se base sur l’algorithme Resnik:\n",
    "•\tsynset1.jcn_similarity(synset2, ic): qui se base sur l’algorithme Jiang-Conrath \n",
    "•\tsynset1.lin_similarity(synset2, ic): qui se base sur l’algorithme Lin \n",
    "\n",
    "\n",
    "l'exemple suivant montre comment calculer les similarité entres les sens des termes computer et device en se basant sur les metriques Leacock-Chodorow et Wu-Palmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('survey.n.01'), Synset('study.n.02'), Synset('report.n.01'), Synset('study.n.04'), Synset('study.n.05'), Synset('discipline.n.01'), Synset('sketch.n.01'), Synset('cogitation.n.02'), Synset('study.n.09'), Synset('study.n.10'), Synset('analyze.v.01'), Synset('study.v.02'), Synset('study.v.03'), Synset('learn.v.04'), Synset('study.v.05'), Synset('study.v.06')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "computer_synsets = wn.synsets(\"computer\") \n",
    "device_synsets = wn.synsets(\"study\") \n",
    "lch=[]\n",
    "wup=[]\n",
    "print(device_synsets)\n",
    "\n",
    "for s1 in computer_synsets:\n",
    "    for s2 in device_synsets:\n",
    "        lch.append(s1.path_similarity(s2))\n",
    "        wup.append(s1.wup_similarity(s2))\n",
    "\n",
    "#pd.DataFrame([lch,wup],[\"lch\",\"wup\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Souvent on aura besoin de recuperer les sens exactes des termes dans leurs contextes afin mesurer leurs similarité d'une manière plus precise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a machine for performing calculations automatically\n",
      "a machine for performing calculations automatically\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def WSD(word, doc):\n",
    "    context= word_tokenize(doc)\n",
    "    sens=lesk(context, word)\n",
    "    return sens\n",
    "\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "\n",
    "\n",
    "print(WSD(\"Computer\", doc1).definition())\n",
    "print(WSD(\"Computer\", doc2).definition())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour comparer calculer la distance semantique entre deux documents, on aura besoin d'un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defnir la fonction SemanticDistanceDocs(doc1,doc2) qui permet de calculer la distance semantique totale entre deux documents texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voici la fonction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def SemanticDistanceDocs2(doc1,doc2):\n",
    "    words=word_tokenize(doc1)\n",
    "    ww=[word.lower() for word in words]\n",
    "    wl= [word for word in ww if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "    words=word_tokenize(doc2)\n",
    "    ww=[word.lower() for word in words]\n",
    "    wl2= [word for word in ww if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "    s1=set(wl)\n",
    "    s2=set(wl2)\n",
    "    S=s1.union(s2)\n",
    "    v1=[]\n",
    "    v2=[]\n",
    "    #print(S)\n",
    "    #print(type(S))\n",
    "    for i in S:\n",
    "        if(i in s1):\n",
    "            v1.append(1.0)\n",
    "        else:\n",
    "            syn1 = WSD(i,doc2)\n",
    "            t=0.0\n",
    "            for l in s1:\n",
    "                temp=[]\n",
    "                \n",
    "                #print(l)\n",
    "                syn2=WSD(l,doc1)\n",
    "                if syn1 is not None and syn2 is not None:\n",
    "                    \n",
    "                    x=syn1.path_similarity(syn2)\n",
    "                else:\n",
    "                    x=0.0\n",
    "                if x is not None:\n",
    "                    if x>t:\n",
    "                        t=x\n",
    "                \n",
    "            y=t\n",
    "            #print(type(y))\n",
    "            if y is not None:\n",
    "                if y>0.25:\n",
    "                    v1.append(y)\n",
    "            else:\n",
    "                v1.append(0)\n",
    "    for i in S:\n",
    "        if(i in s2):\n",
    "            v2.append(1.0)\n",
    "        else:\n",
    "            syn1 = WSD(i,doc1) \n",
    "            for s in s2:\n",
    "                temp=[]\n",
    "                t=0.0\n",
    "                syn2=WSD(l,doc2)\n",
    "                if syn2 is not None and syn2 is not None:\n",
    "                    \n",
    "                    x=syn2.path_similarity(syn2)\n",
    "                else:\n",
    "                    x=0.0\n",
    "                if x is not None:\n",
    "                    if x>t:\n",
    "                        t=x\n",
    "                \n",
    "            y=t\n",
    "            #print(type(y))\n",
    "            if y is not None:\n",
    "                if y>0.25:\n",
    "                    v2.append(y)\n",
    "            else:\n",
    "                v2.append(0)\n",
    "    res=sum(i[0]*i[1] for i in zip(v1,v2))\n",
    "    return res/len(S)\n",
    "    \n",
    "                        \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "doc1='Computer is the study of technology'\n",
    "doc2='Computer is the study of intelligence'\n",
    "#def SemanticDistanceDocs(doc1,doc2):\n",
    "    \n",
    "#print(SemanticDistanceDocs(doc1,doc2))\n",
    "print(SemanticDistanceDocs2(doc1,doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculer les similarités syntaxiques entre les réponses des étudiants et les définitions trouvées sur wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=[]\n",
    "tasks.append(taska)\n",
    "tasks.append(taskb)\n",
    "tasks.append(taskc)\n",
    "tasks.append(taskd)\n",
    "tasks.append(taske)\n",
    "t=corpus[95:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7772727272727272"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SemanticDistanceDocs2(tasks[2][0],t[2]) #exple d'application comparaison entre reponse du task c grp 0\n",
    "#avec la vrai réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "tasks=[]\n",
    "tasks.append(taska)\n",
    "tasks.append(taskb)\n",
    "tasks.append(taskc)\n",
    "tasks.append(taskd)\n",
    "tasks.append(taske)\n",
    "t=corpus[95:]\n",
    "for k in range(5):\n",
    "    print(\"#####result for task \"+chr(ord('a')+k)+\"##########\")\n",
    "    l=-1\n",
    "    b=0\n",
    "    tp={}\n",
    "    for t in range(19):\n",
    "        if t%5==0:\n",
    "            b=0\n",
    "            l+=1\n",
    "        tp[\"Groupe \"+str(l)+\"e\"+chr(ord('A')+b)+\":\"]=SemanticDistanceDocs2(tasks[k][t],t[k])\n",
    "        b+=1\n",
    "    tab.append(tp)\n",
    "    print(tp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
